{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Tensorflow project using MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, MeanAbsoluteError, MeanSquaredError\n",
    "from tensorflow.keras.metrics import Mean, SparseCategoricalAccuracy\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam, SGD, Adagrad\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Rescale to range 0..1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Add channels\n",
    "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
    "x_test = x_test[..., tf.newaxis].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shuffle dataset and prepare batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "training_dataset = training_dataset.shuffle(buffer_size=training_dataset.cardinality()).batch(32)\n",
    "\n",
    "testing_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_1conv(Model):\n",
    "    # Constructor\n",
    "    def __init__(self, n_of_filters, n_of_dense_neurons):\n",
    "        super(model_1conv, self).__init__()\n",
    "\n",
    "        self.conv = Conv2D(filters=n_of_filters,\n",
    "                           kernel_size=3,\n",
    "                           activation=\"relu\")\n",
    "        self.flatten = Flatten()\n",
    "        self.dense_1 = Dense(n_of_dense_neurons, activation=\"relu\")\n",
    "        self.dense_2 = Dense(10)\n",
    "\n",
    "    # Call method\n",
    "    def call(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_2conv(Model):\n",
    "    # Constructor\n",
    "    def __init__(self, n_of_filters, n_of_dense_neurons):\n",
    "        super(model_2conv, self).__init__()\n",
    "\n",
    "        self.conv_1 = Conv2D(filters=n_of_filters[0],\n",
    "                             kernel_size=3,\n",
    "                             activation=\"relu\")\n",
    "        self.conv_1 = Conv2D(filters=n_of_filters[1],\n",
    "                             kernel_size=3,\n",
    "                             activation=\"relu\")\n",
    "        self.flatten = Flatten()\n",
    "        self.dense_1 = Dense(n_of_dense_neurons, activation=\"relu\")\n",
    "        self.dense_2 = Dense(10)\n",
    "\n",
    "    # Call method\n",
    "    def call(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create instances of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different model parameters\n",
    "filters = [4, 8, 16, 32, 64]\n",
    "dense_neurons = [16, 32, 64, 128, 256]\n",
    "\n",
    "# Models with one Convolutional Layer\n",
    "conv1_var_f = [model_1conv(n_of_filters=f, n_of_dense_neurons=128) for f in filters]            # Variated filters\n",
    "conv1_var_d = [model_1conv(n_of_filters=32, n_of_dense_neurons=dn) for dn in dense_neurons]     # Variated number of neurons in Dense layer\n",
    "\n",
    "# Model with two Convolutional Layers\n",
    "conv2_var_f = [model_2conv(n_of_filters=(f, f), n_of_dense_neurons=128) for f in filters]               # Variated filters\n",
    "conv2_var_f = [model_2conv(n_of_filters=(32, 32), n_of_dense_neurons=dn) for dn in dense_neurons]       # Variated number of neurons in Dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss functions and optimizers for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_objects = [MeanAbsoluteError(), MeanSquaredError(), SparseCategoricalCrossentropy(from_logits=True)]\n",
    "optimizers = [Adam(), SGD(), Adagrad()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for measurement of model's loss and accuracy\n",
    "training_loss = Mean(name=\"train_loss\")\n",
    "testing_loss = Mean(name=\"test_loss\")\n",
    "\n",
    "training_accuracy = SparseCategoricalAccuracy(name=\"train_acc\")\n",
    "testing_accuracy = SparseCategoricalAccuracy(name=\"test_acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model's training and testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function as Tensorflow Graph\n",
    "# @tf.function\n",
    "def train_step(model, loss_object, optimizer, images, labels):\n",
    "\n",
    "    # Automatic differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    training_loss(loss)\n",
    "    training_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function as Tensorflow Graph\n",
    "# @tf.function\n",
    "def test_step(model, loss_object, images, labels):\n",
    "    predictions = model(images, training=False)\n",
    "    loss = loss_object(labels, predictions)\n",
    "\n",
    "    testing_loss(loss)\n",
    "    testing_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training and model metrics acquisition\n",
    "\n",
    "The deafult model has one Convolutional Layer with 32 filters and a Dense Layer with 128 neurons, uses SparseCategoricalCrossentropy to calculate loss and uses Adam optimizer.\n",
    "\n",
    "I decided to compare models after changing one of the following aspects:\n",
    "\n",
    "- Number of filters used in Convolutional Layers\n",
    "- Number of neurons used in Dense Layers\n",
    "- What impact will adding one additional Convolutional Layer have (and if previously mentions aspects matter after adding it)\n",
    "- Type of Loss Function used\n",
    "- Type of optimizer used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data structure for metrics\n",
    "cols = [\"train_loss\", \"train_acc\", \"test_loss\", \"test_acc\"]\n",
    "metrics = {f\"{i}_filters\": pd.DataFrame(columns=cols) for i in filters}\n",
    "epoch_mean_training_time = pd.DataFrame(columns=list(map(str, filters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with 4 filters:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patry\\AppData\\Local\\Temp\\ipykernel_1620\\3896342143.py:41: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  metrics[f\"{filters[i]}_filters\"] = pd.concat([metrics[f\"{filters[i]}_filters\"], pd.DataFrame([record])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1,     Loss: 0.1761493980884552,     Accuracy: 95.74333190917969,     Test Loss: 0.16163507103919983,     Test Accuracy: 95.62000274658203\n",
      "    Epoch 2,     Loss: 0.17160136997699738,     Accuracy: 96.19166564941406,     Test Loss: 0.1425904780626297,     Test Accuracy: 96.52000427246094\n",
      "    Epoch 3,     Loss: 0.16985194385051727,     Accuracy: 96.45500183105469,     Test Loss: 0.15842370688915253,     Test Accuracy: 96.24000549316406\n",
      "    Epoch 4,     Loss: 0.1851879209280014,     Accuracy: 96.92500305175781,     Test Loss: 0.1590336263179779,     Test Accuracy: 96.86000061035156\n",
      "    Epoch 5,     Loss: 0.1717226207256317,     Accuracy: 97.32333374023438,     Test Loss: 0.137295201420784,     Test Accuracy: 97.5999984741211\n",
      "\n",
      "\n",
      "{'4_filters':    train_loss  train_acc  test_loss  test_acc\n",
      "0    0.176149   0.957433   0.161635    0.9562\n",
      "1    0.171601   0.961917   0.142590    0.9652\n",
      "2    0.169852   0.964550   0.158424    0.9624\n",
      "3    0.185188   0.969250   0.159034    0.9686\n",
      "4    0.171723   0.973233   0.137295    0.9760, '8_filters': Empty DataFrame\n",
      "Columns: [train_loss, train_acc, test_loss, test_acc]\n",
      "Index: [], '16_filters': Empty DataFrame\n",
      "Columns: [train_loss, train_acc, test_loss, test_acc]\n",
      "Index: [], '32_filters': Empty DataFrame\n",
      "Columns: [train_loss, train_acc, test_loss, test_acc]\n",
      "Index: [], '64_filters': Empty DataFrame\n",
      "Columns: [train_loss, train_acc, test_loss, test_acc]\n",
      "Index: []}\n",
      "\n",
      "\n",
      "119.11579313278199\n"
     ]
    }
   ],
   "source": [
    "N_OF_EPOCHS = 5\n",
    "\n",
    "mean_times = []\n",
    "\n",
    "# Training and testing of models with one Convolutional Layer and variated number of filters\n",
    "for i, model in enumerate(conv1_var_f):       # [4, 8, 16, 32, 64]\n",
    "    print(f\"Model with {filters[i]} filters:\")\n",
    "    training_times = []\n",
    "\n",
    "    for epoch in range(N_OF_EPOCHS):\n",
    "\n",
    "        # Metrics reset\n",
    "        training_loss.reset_states()\n",
    "        testing_loss.reset_states()\n",
    "        training_accuracy.reset_states()\n",
    "        testing_accuracy.reset_states()\n",
    "\n",
    "        # Training\n",
    "        t_start = time.time()\n",
    "        for train_images, train_labels in training_dataset:\n",
    "            train_step(model=model,\n",
    "                       loss_object=SparseCategoricalCrossentropy(from_logits=True),\n",
    "                       optimizer=Adam(),\n",
    "                       images=train_images,\n",
    "                       labels=train_labels)\n",
    "        t_stop = time.time()\n",
    "\n",
    "        # Testing\n",
    "        for test_images, test_labels in testing_dataset:\n",
    "            test_step(model=model,\n",
    "                      loss_object=SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      images=test_images,\n",
    "                      labels=test_labels)\n",
    "\n",
    "        record = {\"train_loss\": training_loss.result().numpy(),\n",
    "                  \"train_acc\": training_accuracy.result().numpy(),\n",
    "                  \"test_loss\": testing_loss.result().numpy(),\n",
    "                  \"test_acc\": testing_accuracy.result().numpy()}\n",
    "        \n",
    "        # Save metrics and training time for this epoch\n",
    "        metrics[f\"{filters[i]}_filters\"] = pd.concat([metrics[f\"{filters[i]}_filters\"], pd.DataFrame([record])], ignore_index=True)\n",
    "        training_times.append(t_stop - t_start)\n",
    "        # metrics = pd.concat([metrics, pd.DataFrame([record])], ignore_index=True)\n",
    "\n",
    "        print(f\"    Epoch {epoch + 1}, \"\n",
    "              f\"    Loss: {training_loss.result()}, \"\n",
    "              f\"    Accuracy: {training_accuracy.result() * 100}, \"\n",
    "              f\"    Test Loss: {testing_loss.result()}, \"\n",
    "              f\"    Test Accuracy: {testing_accuracy.result() * 100}\")\n",
    "    \n",
    "    # Save mean training time\n",
    "    mean_times.append(np.mean(np.array(training_times)))\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(metrics)\n",
    "    print(\"\\n\")\n",
    "    print(mean_times)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
